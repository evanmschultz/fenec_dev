{
    "function_name": "_get_chroma_queries",
    "docstring": "Generates Chroma queries based on the user question.\n\nArgs:\n    - user_question (str): The user's question.\n    - queries_count (int, optional): Number of queries to generate. Defaults to 3.\n    - retries (int, optional): Number of retries in case of failure. Defaults to 3.\n\nReturns:\n    - list[str] | None: The generated list of Chroma queries, or None if unsuccessful.",
    "decorators": null,
    "parameters": null,
    "returns": "list[str] | None",
    "is_method": true,
    "is_async": false,
    "id": "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-_get_chroma_queries",
    "file_path": "fenec/ai_services/librarians/chroma_librarians.py",
    "parent_id": "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian",
    "block_type": "FUNCTION",
    "start_line_num": 136,
    "end_line_num": 195,
    "code_content": "\ndef _get_chroma_queries(\n    self, user_question: str, queries_count: int = 3, retries: int = 3\n) -> list[str] | None:\n    \"\"\"\n        Generates Chroma queries based on the user question.\n\n        Args:\n            - user_question (str): The user's question.\n            - queries_count (int, optional): Number of queries to generate. Defaults to 3.\n            - retries (int, optional): Number of retries in case of failure. Defaults to 3.\n\n        Returns:\n            - list[str] | None: The generated list of Chroma queries, or None if unsuccessful.\n        \"\"\"\n\n    while retries > 0:\n        retries -= 1\n\n        prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n            user_question,\n            prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n            queries_count=queries_count,\n        )\n\n        try:\n            completion: openai_types.ChatCompletion = (\n                self.client.chat.completions.create(\n                    model=self.model,\n                    response_format={\"type\": \"json_object\"},\n                    messages=[\n                        {\n                            \"role\": \"system\",\n                            \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                        },\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                )\n            )\n            content: str | None = completion.choices[0].message.content\n            if not content:\n                continue\n\n            content_json = json.loads(content)\n            content_model = OpenAIResponseContent(\n                query_list=content_json[\"query_list\"]\n            )\n            content_model.query_list.append(user_question)\n            queries_count += 1\n\n            if content:\n                queries: list[str] = content_model.query_list\n                if queries and len(queries) == queries_count:\n                    return queries\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n    return None\n",
    "important_comments": null,
    "dependencies": null,
    "summary": "This code defines a function `_get_chroma_queries` that is designed to generate a list of Chroma queries based on a user's input question, with configurable parameters for the number of queries and retry attempts. The primary purpose of this function is to enhance user interaction by generating contextually relevant search queries, which can be used in a search or recommendation engine to improve user experience. Key components of the function include a retry loop that attempts to generate queries up to a specified number of times; a prompt creation process using `ChromaLibrarianPromptCreator.create_prompt`, which formats the user's question and desired query count using a default template; and an API call to OpenAI's chat completion service to obtain query suggestions. The implementation involves constructing a prompt with the user's question, sending it to the OpenAI API, and parsing the JSON response to extract a list of queries. This list is then validated to ensure it matches the expected count before being returned. If the process fails, the function retries until the retry limit is reached.\n\nThe technical stack includes OpenAI's API for generating chat completions, which is crucial for leveraging advanced language models to generate relevant queries; JSON for parsing the API responses, ensuring structured data handling; and a custom utility for prompt creation, which standardizes the input format for the API. The function also utilizes a custom data model, `OpenAIResponseContent`, to manage the query list and ensure it includes the user's original question.\n\nIn the context of a larger system, this function likely serves as a backend component for a user interaction platform, potentially interfacing with a search or recommendation engine. It contributes to the system by providing contextually relevant suggestions, thereby enhancing the overall user experience. This function is part of a broader architecture that may include user interface components, data storage systems, and other AI-driven modules, working together to deliver a seamless and intelligent user interaction platform. The modular design of the function allows it to be easily integrated into various applications, supporting dynamic query generation and improving the relevance of search results or recommendations.",
    "children_ids": []
}