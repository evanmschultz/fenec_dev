{
    "class_name": "OllamaSummarizationConfigs",
    "decorators": null,
    "bases": [
        "SummarizationConfigs",
        "BaseModel"
    ],
    "docstring": "Configs for the Ollama completion.\n\nUsed to set the chat completion parameters for the Ollama chat completions method call.\n\nArgs:\n    - `model` (str): The model to use for the completion. Default is \"gpt-4o\".\n    - `max_tokens` (int | None): The maximum number of tokens to generate. 'None' implies no limit. Default is None.\n    - `stream` (bool): Whether to stream back partial progress. Default is False.\n    - `temperature` (float): Sampling temperature to use. Default is 0.0.\n\nNotes:\n    - `model` must be a valid Ollama model name with a valid parameter syntax.\n\nExamples:\n    ```Python\n    ollama_completion_configs = OllamaConfigs(\n        model=\"codellama:13b-python\",\n    ```",
    "keywords": null,
    "id": "fenec:utilities:configs:configs.py__*__MODULE__*__CLASS-OllamaSummarizationConfigs",
    "file_path": "fenec/utilities/configs/configs.py",
    "parent_id": "fenec:utilities:configs:configs.py__*__MODULE",
    "block_type": "CLASS",
    "start_line_num": 143,
    "end_line_num": 169,
    "code_content": "\n\nclass OllamaSummarizationConfigs(SummarizationConfigs, BaseModel):\n    \"\"\"\n    Configs for the Ollama completion.\n\n    Used to set the chat completion parameters for the Ollama chat completions method call.\n\n    Args:\n        - `model` (str): The model to use for the completion. Default is \"gpt-4o\".\n        - `max_tokens` (int | None): The maximum number of tokens to generate. 'None' implies no limit. Default is None.\n        - `stream` (bool): Whether to stream back partial progress. Default is False.\n        - `temperature` (float): Sampling temperature to use. Default is 0.0.\n\n    Notes:\n        - `model` must be a valid Ollama model name with a valid parameter syntax.\n\n    Examples:\n        ```Python\n        ollama_completion_configs = OllamaConfigs(\n            model=\"codellama:13b-python\",\n        ```\n    \"\"\"\n\n    model: str = \"codellama:7b\"\n    system_message: str = summarization_prompts.SUMMARIZER_DEFAULT_INSTRUCTIONS\n",
    "important_comments": null,
    "dependencies": [
        {
            "code_block_id": "fenec:utilities:configs:configs.py__*__MODULE__*__CLASS-SummarizationConfigs"
        },
        {
            "import_names": [
                {
                    "name": "BaseModel",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "pydantic",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "summarization_prompts",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "fenec.ai_services.summarizer.prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:summarizer:prompts:summarization_prompts.py__*__MODULE"
        }
    ],
    "summary": "The `OllamaSummarizationConfigs` class is a configuration utility designed to facilitate the setup of parameters for the Ollama chat completion method, specifically tailored for summarization tasks. Its primary purpose is to provide a structured and flexible mechanism for configuring and managing the parameters necessary for generating text completions using the Ollama API, which is crucial for applications that require customizable and dynamic text generation. This class inherits from both `SummarizationConfigs` and `BaseModel`, indicating its integration into a broader framework that likely encompasses multiple configuration classes for various text processing tasks. Key components of this class include several attributes: `model`, which specifies the model to be used for text generation, defaulting to \"codellama:7b\"; `max_tokens`, which sets a limit on the number of tokens generated, with `None` indicating no limit; `stream`, a boolean that determines if partial progress should be streamed back to the user, defaulting to `False`; and `temperature`, which controls the randomness of the output, set to a default of 0.0 for deterministic results. The `system_message` attribute is initialized with a default summarization instruction from `summarization_prompts.SUMMARIZER_DEFAULT_INSTRUCTIONS`, ensuring consistent guidance for the summarization process.\n\nThe implementation leverages Python's type hinting to enforce correct data types for each attribute, enhancing code reliability and maintainability. Default values are provided to facilitate ease of use and quick setup for users. The class design follows principles of object-oriented programming, promoting reusability and modularity. The use of `BaseModel` suggests reliance on a library like Pydantic, which is employed for data validation and management, ensuring that the configuration parameters adhere to expected formats and constraints.\n\nIn terms of the technical stack, the code is built on Python, with dependencies on Pydantic for data validation and possibly other internal modules like `summarization_prompts` for default instructions. This setup indicates a robust framework designed to handle various text generation tasks with flexibility and precision.\n\nWithin the larger project or system, this configuration class is designed to integrate seamlessly with the Ollama API, allowing users to specify and adjust parameters for text generation tasks. It enhances the flexibility and control over the summarization process, making it a vital component for applications that require dynamic and customizable text generation capabilities. This class would typically interact with other components responsible for executing API calls, managing user inputs, and handling the generated outputs, thereby playing a critical role in the overall text processing pipeline. Its design supports scalability and adaptability, making it suitable for diverse use cases in natural language processing applications.",
    "children_ids": []
}