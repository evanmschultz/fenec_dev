{
    "class_name": "OpenAIChatAgent",
    "decorators": null,
    "bases": null,
    "docstring": "Represents an agent that interacts with the OpenAI API for generating responses to user questions.\n\nArgs:\n    - `chroma_librarian` (ChromaLibrarian): The librarian handling Chroma queries.\n    - `configs` (OpenAIConfigs, optional): Configuration settings for the OpenAI agent.\n\nMethods:\n    - `get_response`(user_question, prompt_template=DEFAULT_PROMPT_TEMPLATE):\n        Generates a response to the user's question using the specified prompt template.\n\n\n\nAttributes:\n    - `chroma_librarian` (ChromaLibrarian): The Chroma librarian instance.\n    - `model` (str): The OpenAI model being used.\n    - `client`: The OpenAI API client.",
    "keywords": null,
    "id": "fenec:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent",
    "file_path": "fenec/ai_services/chat/openai_agents.py",
    "parent_id": "fenec:ai_services:chat:openai_agents.py__*__MODULE",
    "block_type": "CLASS",
    "start_line_num": 14,
    "end_line_num": 148,
    "code_content": "\n\nclass OpenAIChatAgent:\n    \"\"\"\n    Represents an agent that interacts with the OpenAI API for generating responses to user questions.\n\n    Args:\n        - `chroma_librarian` (ChromaLibrarian): The librarian handling Chroma queries.\n        - `configs` (OpenAIConfigs, optional): Configuration settings for the OpenAI agent.\n\n    Methods:\n        - `get_response`(user_question, prompt_template=DEFAULT_PROMPT_TEMPLATE):\n            Generates a response to the user's question using the specified prompt template.\n\n\n\n    Attributes:\n        - `chroma_librarian` (ChromaLibrarian): The Chroma librarian instance.\n        - `model` (str): The OpenAI model being used.\n        - `client`: The OpenAI API client.\n    \"\"\"\n\n    def __init__(\n        self,\n        chroma_librarian: ChromaLibrarian,\n        configs: OpenAIConfigs = OpenAIConfigs(),\n    ) -> None:\n        self.chroma_librarian: ChromaLibrarian = chroma_librarian\n        self.configs: OpenAIConfigs = configs\n        self.client = OpenAI()\n\n    def get_response(\n        self, user_question: str, prompt_template: str = DEFAULT_PROMPT_TEMPLATE\n    ) -> str | None:\n        \"\"\"\n        Generates a response to the user's question using the OpenAI API.\n\n        Args:\n            - `user_question` (str): The user's question.\n            - `prompt_template` (str, optional): The template for formatting the prompt.\n                default: DEFAULT_PROMPT_TEMPLATE.\n\n        Returns:\n            - `str | None`: The generated response or None if the response could not be generated.\n\n        Raises:\n            - `ValueError`: If user_question is empty.\n            - `RuntimeError`: If there is an issue with the OpenAI API request.\n            - `KeyError`: If the prompt template is missing required keys.\n\n        Example:\n            ```python\n            agent = OpenAIChatAgent(chroma_librarian, model=\"gpt-4o\")\n            try:\n                response = agent.get_response(\"What code blocks use recursion?\")\n                print(response)\n            except ValueError as ve:\n                print(f\"ValueError: {ve}\")\n            except RuntimeError as re:\n                print(f\"RuntimeError: {re}\")\n            except KeyError as ke:\n                print(f\"KeyError: {ke}\")\n            ```\n        \"\"\"\n        if not user_question:\n            raise ValueError(\"User question cannot be empty.\")\n\n        try:\n            chroma_results: chroma_types.QueryResult | None = (\n                self.chroma_librarian.query_chroma(user_question)\n            )\n\n            if not chroma_results:\n                return \"I don't know how to answer that question.\"\n\n            documents: list[list[str]] | None = chroma_results[\"documents\"]\n\n            if not documents:\n                return \"I don't know how to answer that question.\"\n\n            context: str = \"\"\n            for document in documents:\n                context += \"\\n\".join(document) + \"\\n\"\n\n            prompt: str = self._format_prompt(context, user_question, prompt_template)\n\n            messages: Sequence[dict[str, str]] = [\n                {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n            response: openai_types.ChatCompletion = self.client.chat.completions.create(\n                model=self.configs.model,\n                messages=messages,  # type: ignore # FIXME: fix type hinting error\n                temperature=self.configs.temperature,\n                # response_format={\"type\": \"json_object\"},\n            )\n            return response.choices[0].message.content\n\n        except Exception as e:\n            raise RuntimeError(f\"Error interacting with OpenAI API: {e}\") from e\n\n    def _format_prompt(\n        self,\n        context: str,\n        user_question: str,\n        prompt_template: str,\n    ) -> str:\n        \"\"\"\n        Formats the prompt for the OpenAI API based on the provided context, user's question, and template.\n\n        Args:\n            - `context` (str): The context derived from Chroma query results.\n            - `user_question` (str): The user's question.\n            - `prompt_template` (str): The template for formatting the prompt.\n\n        Returns:\n            - `str`: The formatted prompt.\n\n        Raises:\n            - `KeyError`: If the prompt template is missing required keys.\n\n        Example:\n            ```python\n            prompt = agent._format_prompt(\"Context here\", \"What is the meaning of life?\", \"Template {context} {user_question}\")\n            print(prompt)\n            ```\n        \"\"\"\n\n        try:\n            return prompt_template.format(context=context, user_question=user_question)\n\n        except KeyError as e:\n            raise KeyError(f\"Prompt template is missing the following key: {e}\") from e\n",
    "important_comments": null,
    "dependencies": [
        {
            "import_names": [
                {
                    "name": "Sequence",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "typing",
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAI",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "openai",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAIConfigs",
                    "as_name": null,
                    "local_block_id": "fenec:utilities:configs:configs.py__*__MODULE__*__CLASS-OpenAIConfigs"
                }
            ],
            "imported_from": "fenec.utilities.configs.configs",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:utilities:configs:configs.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:chroma.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.openai",
                    "as_name": "openai_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:openai.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarian",
                    "as_name": null,
                    "local_block_id": "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian"
                }
            ],
            "imported_from": "fenec.ai_services.librarians.chroma_librarians",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_PROMPT_TEMPLATE",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "fenec.ai_services.chat.prompts.chat_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:chat:prompts:chat_prompts.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_PROMPT_TEMPLATE",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "fenec.ai_services.chat.prompts.chat_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:chat:prompts:chat_prompts.py__*__MODULE"
        }
    ],
    "summary": "The `OpenAIChatAgent` class is designed to facilitate advanced interactions with the OpenAI API, enhancing the relevance and accuracy of AI-generated responses by integrating contextual information retrieved from a Chroma database. Its primary purpose is to generate context-aware responses to user queries by leveraging external data, thus improving the quality of conversational AI applications. Key components of this class include the `__init__` method, which initializes the agent with a `ChromaLibrarian` instance for context retrieval and an optional `OpenAIConfigs` object for configuration management; the `get_response` method, which processes user questions, queries the Chroma database for relevant context, formats the prompt using a specified template, and retrieves a response from the OpenAI API; and the `_format_prompt` method, which constructs the prompt by incorporating the context and user question into a predefined template.\n\nThe implementation involves querying the Chroma database through the `ChromaLibrarian` to obtain context relevant to the user's question. This context is then formatted into a prompt using the `_format_prompt` method, which employs Python's string formatting capabilities to integrate the context and user question into the template. The formatted prompt is sent to the OpenAI API via the `client.chat.completions.create` method, which returns a response object. The method handles various exceptions, such as empty user questions, API request issues, and missing template keys, ensuring robust error management and user feedback. The design pattern emphasizes separation of concerns, where each component has a distinct role, contributing to a clean and maintainable codebase.\n\nThe technical stack includes the OpenAI API, which is used for generating responses, and the `ChromaLibrarian`, which is responsible for querying the Chroma database to retrieve context. The `OpenAIConfigs` class manages configuration settings, such as the model to be used. The code also utilizes Python's type hinting and exception handling to ensure type safety and error robustness. This setup suggests a design pattern that emphasizes modularity and organized management of API interactions.\n\nIn the context of a larger system, the `OpenAIChatAgent` acts as a bridge between user input and the OpenAI model, enhancing the interaction by incorporating context from external sources. It integrates with other components that manage data retrieval and configuration, playing a crucial role in a system designed to provide intelligent, context-aware responses. This integration allows the system to deliver more accurate and relevant information, improving the overall user experience and effectiveness of the AI model in various applications, such as customer support, information retrieval, and interactive AI systems. The class likely operates within a broader architecture that includes user interface components, additional data processing modules, and possibly other AI models, contributing to a seamless user experience in applications such as virtual assistants, customer support bots, or educational tools.",
    "children_ids": [
        "fenec:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent__*__FUNCTION-__init__",
        "fenec:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent__*__FUNCTION-get_response",
        "fenec:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent__*__FUNCTION-_format_prompt"
    ]
}