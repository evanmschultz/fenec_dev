{
    "docstring": null,
    "header": [],
    "footer": [],
    "imports": [
        {
            "import_names": [
                {
                    "name": "logging",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "Sequence",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "typing",
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAI",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "openai",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAIConfigs",
                    "as_name": null,
                    "local_block_id": "fenec:utilities:configs:configs.py__*__MODULE__*__CLASS-OpenAIConfigs"
                }
            ],
            "imported_from": "fenec.utilities.configs.configs",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:utilities:configs:configs.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:chroma.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.openai",
                    "as_name": "openai_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:openai.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarian",
                    "as_name": null,
                    "local_block_id": "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian"
                }
            ],
            "imported_from": "fenec.ai_services.librarians.chroma_librarians",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_PROMPT_TEMPLATE",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "fenec.ai_services.chat.prompts.chat_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:chat:prompts:chat_prompts.py__*__MODULE"
        }
    ],
    "id": "fenec:ai_services:chat:openai_agents.py__*__MODULE",
    "file_path": "fenec/ai_services/chat/openai_agents.py",
    "parent_id": "fenec:ai_services:chat__*__DIRECTORY",
    "block_type": "MODULE",
    "start_line_num": 1,
    "end_line_num": 148,
    "code_content": "import logging\nfrom typing import Sequence\nfrom openai import OpenAI\n\nfrom fenec.utilities.configs.configs import OpenAIConfigs\nimport fenec.types.chroma as chroma_types\nimport fenec.types.openai as openai_types\n\nfrom fenec.ai_services.librarians.chroma_librarians import ChromaLibrarian\nfrom fenec.ai_services.chat.prompts.chat_prompts import (\n    DEFAULT_PROMPT_TEMPLATE,\n    DEFAULT_SYSTEM_PROMPT,\n)\n\n\nclass OpenAIChatAgent:\n    \"\"\"\n    Represents an agent that interacts with the OpenAI API for generating responses to user questions.\n\n    Args:\n        - `chroma_librarian` (ChromaLibrarian): The librarian handling Chroma queries.\n        - `configs` (OpenAIConfigs, optional): Configuration settings for the OpenAI agent.\n\n    Methods:\n        - `get_response`(user_question, prompt_template=DEFAULT_PROMPT_TEMPLATE):\n            Generates a response to the user's question using the specified prompt template.\n\n\n\n    Attributes:\n        - `chroma_librarian` (ChromaLibrarian): The Chroma librarian instance.\n        - `model` (str): The OpenAI model being used.\n        - `client`: The OpenAI API client.\n    \"\"\"\n\n    def __init__(\n        self,\n        chroma_librarian: ChromaLibrarian,\n        configs: OpenAIConfigs = OpenAIConfigs(),\n    ) -> None:\n        self.chroma_librarian: ChromaLibrarian = chroma_librarian\n        self.configs: OpenAIConfigs = configs\n        self.client = OpenAI()\n\n    def get_response(\n        self, user_question: str, prompt_template: str = DEFAULT_PROMPT_TEMPLATE\n    ) -> str | None:\n        \"\"\"\n        Generates a response to the user's question using the OpenAI API.\n\n        Args:\n            - `user_question` (str): The user's question.\n            - `prompt_template` (str, optional): The template for formatting the prompt.\n                default: DEFAULT_PROMPT_TEMPLATE.\n\n        Returns:\n            - `str | None`: The generated response or None if the response could not be generated.\n\n        Raises:\n            - `ValueError`: If user_question is empty.\n            - `RuntimeError`: If there is an issue with the OpenAI API request.\n            - `KeyError`: If the prompt template is missing required keys.\n\n        Example:\n            ```python\n            agent = OpenAIChatAgent(chroma_librarian, model=\"gpt-4o\")\n            try:\n                response = agent.get_response(\"What code blocks use recursion?\")\n                print(response)\n            except ValueError as ve:\n                print(f\"ValueError: {ve}\")\n            except RuntimeError as re:\n                print(f\"RuntimeError: {re}\")\n            except KeyError as ke:\n                print(f\"KeyError: {ke}\")\n            ```\n        \"\"\"\n        if not user_question:\n            raise ValueError(\"User question cannot be empty.\")\n\n        try:\n            chroma_results: chroma_types.QueryResult | None = (\n                self.chroma_librarian.query_chroma(user_question)\n            )\n\n            if not chroma_results:\n                return \"I don't know how to answer that question.\"\n\n            documents: list[list[str]] | None = chroma_results[\"documents\"]\n\n            if not documents:\n                return \"I don't know how to answer that question.\"\n\n            context: str = \"\"\n            for document in documents:\n                context += \"\\n\".join(document) + \"\\n\"\n\n            prompt: str = self._format_prompt(context, user_question, prompt_template)\n\n            messages: Sequence[dict[str, str]] = [\n                {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n            response: openai_types.ChatCompletion = self.client.chat.completions.create(\n                model=self.configs.model,\n                messages=messages,  # type: ignore # FIXME: fix type hinting error\n                temperature=self.configs.temperature,\n                # response_format={\"type\": \"json_object\"},\n            )\n            return response.choices[0].message.content\n\n        except Exception as e:\n            raise RuntimeError(f\"Error interacting with OpenAI API: {e}\") from e\n\n    def _format_prompt(\n        self,\n        context: str,\n        user_question: str,\n        prompt_template: str,\n    ) -> str:\n        \"\"\"\n        Formats the prompt for the OpenAI API based on the provided context, user's question, and template.\n\n        Args:\n            - `context` (str): The context derived from Chroma query results.\n            - `user_question` (str): The user's question.\n            - `prompt_template` (str): The template for formatting the prompt.\n\n        Returns:\n            - `str`: The formatted prompt.\n\n        Raises:\n            - `KeyError`: If the prompt template is missing required keys.\n\n        Example:\n            ```python\n            prompt = agent._format_prompt(\"Context here\", \"What is the meaning of life?\", \"Template {context} {user_question}\")\n            print(prompt)\n            ```\n        \"\"\"\n\n        try:\n            return prompt_template.format(context=context, user_question=user_question)\n\n        except KeyError as e:\n            raise KeyError(f\"Prompt template is missing the following key: {e}\") from e\n",
    "important_comments": null,
    "dependencies": null,
    "summary": "The `OpenAIChatAgent` class is a sophisticated component designed to enhance AI-generated responses by integrating contextual information from a Chroma database with the OpenAI API. Its primary purpose is to improve the relevance and accuracy of responses to user queries by leveraging external data sources. Key components include the `__init__` method, which initializes the agent with a `ChromaLibrarian` for context retrieval and an optional `OpenAIConfigs` for configuration management; the `get_response` method, which processes user questions, queries the Chroma database for relevant context, formats the prompt using a specified template, and retrieves a response from the OpenAI API; and the `_format_prompt` method, which constructs the prompt by incorporating the context and user question into a predefined template.\n\nThe implementation involves querying the Chroma database through the `ChromaLibrarian` to obtain context relevant to the user's question. This context is formatted into a prompt using the `_format_prompt` method, which employs Python's string formatting capabilities to integrate the context and user question into the template. The formatted prompt is then sent to the OpenAI API via the `client.chat.completions.create` method, which returns a response object. The code includes robust error handling to manage exceptions such as empty user questions, API request issues, and missing template keys, ensuring reliable operation and user feedback.\n\nThe technical stack comprises the OpenAI API, which provides the language model for generating responses; a custom `ChromaLibrarian` for querying the Chroma database to retrieve contextual information; and `OpenAIConfigs` for managing configuration settings related to the OpenAI model and API client. The code also utilizes Python's `logging` module for potential logging purposes, although logging functionality is not explicitly implemented in the provided code snippet. The `fenec` package appears to be a custom library that provides types and utilities for interacting with both Chroma and OpenAI services, indicating a modular design that separates concerns and enhances maintainability.\n\nIn the context of a larger system, this code acts as a bridge between user input and AI-generated responses, integrating with Chroma for context retrieval and OpenAI for language processing. It enhances the system's ability to provide informed and contextually relevant answers by leveraging external data sources. This integration suggests that the `OpenAIChatAgent` is part of a broader AI service architecture, potentially interfacing with other components such as user interfaces, data storage systems, and additional AI models. The modular design and use of configuration management indicate that the system is designed for flexibility and scalability, allowing for easy updates and integration with other services or models.",
    "children_ids": [
        "fenec:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent"
    ]
}