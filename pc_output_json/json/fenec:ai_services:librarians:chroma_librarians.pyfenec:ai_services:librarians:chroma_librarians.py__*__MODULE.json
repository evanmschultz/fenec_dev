{
    "docstring": null,
    "header": [],
    "footer": [],
    "imports": [
        {
            "import_names": [
                {
                    "name": "logging",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "json",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAI",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "openai",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "BaseModel",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "pydantic",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.openai",
                    "as_name": "openai_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:openai.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaCollectionManager",
                    "as_name": null,
                    "local_block_id": "fenec:databases:chroma:chromadb_collection_manager.py__*__MODULE__*__CLASS-ChromaCollectionManager"
                }
            ],
            "imported_from": "fenec.databases.chroma.chromadb_collection_manager",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:databases:chroma:chromadb_collection_manager.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarianPromptCreator",
                    "as_name": null,
                    "local_block_id": "fenec:ai_services:librarians:prompts:prompt_creator.py__*__MODULE__*__CLASS-ChromaLibrarianPromptCreator"
                }
            ],
            "imported_from": "fenec.ai_services.librarians.prompts.prompt_creator",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:librarians:prompts:prompt_creator.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "fenec.ai_services.librarians.prompts.chroma_librarian_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:librarians:prompts:chroma_librarian_prompts.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:chroma.py__*__MODULE"
        }
    ],
    "id": "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE",
    "file_path": "fenec/ai_services/librarians/chroma_librarians.py",
    "parent_id": "fenec:ai_services:librarians__*__DIRECTORY",
    "block_type": "MODULE",
    "start_line_num": 1,
    "end_line_num": 195,
    "code_content": "import logging\nimport json\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport fenec.types.openai as openai_types\nfrom fenec.databases.chroma.chromadb_collection_manager import (\n    ChromaCollectionManager,\n)\nfrom fenec.ai_services.librarians.prompts.prompt_creator import (\n    ChromaLibrarianPromptCreator,\n)\nfrom fenec.ai_services.librarians.prompts.chroma_librarian_prompts import (\n    DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n    DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n)\nimport fenec.types.chroma as chroma_types\n\n# TOOLS: list[dict[str, Any]] = [\n#     {\n#         \"type\": \"function\",\n#         \"function\": {\n#             \"name\": \"query_chroma\",\n#             \"description\": \"Get the results from the chromadb vector database using a list of queries.\",\n#             \"parameters\": {\n#                 \"type\": \"object\",\n#                 \"properties\": {\n#                     \"queries\": {\n#                         \"type\": \"list[str]\",\n#                         \"description\": \"List of queries to use to get the results from the chromadb vector database.\",\n#                     },\n#                     \"n_results\": {\n#                         \"type\": \"int\",\n#                         \"description\": \"Number of results to return, default is 10.\",\n#                     },\n#                 },\n#                 \"required\": [\"queries\"],\n#             },\n#         },\n#     }\n# ]\n\n\nclass OpenAIResponseContent(BaseModel):\n    \"\"\"\n    Pydantic model representing the content structure of an OpenAI response.\n\n    OpenAI is set to respond with a JSON object, so this model is used to parse the response.\n\n    Attributes:\n        - query_list (list[str]): List of queries in the OpenAI response.\n    \"\"\"\n\n    query_list: list[str]\n\n\nclass ChromaLibrarian:\n    def __init__(\n        self,\n        collection_manager: ChromaCollectionManager,\n        model: str = \"gpt-3.5-turbo-1106\",\n    ) -> None:\n        \"\"\"\n        Represents a librarian for interacting with the Chroma database using OpenAI.\n\n        Args:\n            - collection_manager (ChromaCollectionManager): The manager for Chroma collections.\n            - model (str, optional): The OpenAI model to use. Defaults to \"gpt-3.5-turbo-1106\".\n\n        Methods:\n            - query_chroma(user_question):\n                Queries the Chroma database using the provided user question.\n\n            - _query_collection(queries, n_results=3):\n                Queries the Chroma collection manager with a list of queries.\n\n            - _get_chroma_queries(user_question, queries_count=3, retries=3):\n                Generates Chroma queries based on the user question.\n\n        Attributes:\n            - collection_manager (ChromaCollectionManager): The Chroma collection manager.\n            - model (str): The OpenAI model being used.\n            - client: The OpenAI API client.\n\n        Examples:\n            ```python\n            chroma_librarian = ChromaLibrarian(chroma_collection_manager)\n            chroma_librarian.query_chroma(\"Which models are inherited by others?\")\n            ```\n        \"\"\"\n\n        self.collection_manager: ChromaCollectionManager = collection_manager\n        self.model: str = model\n        self.client = OpenAI()\n\n    def query_chroma(self, user_question: str) -> chroma_types.QueryResult | None:\n        \"\"\"\n        Queries the Chroma database using the provided user question.\n\n        Args:\n            - user_question (str): The user's question.\n\n        Returns:\n            - chroma_types.QueryResult | None: The result of the Chroma query, or None if unsuccessful.\n        \"\"\"\n\n        queries: list[str] | None = self._get_chroma_queries(user_question)\n        if not queries:\n            return None\n\n        print(queries)\n\n        return self._query_collection(queries)\n\n    def _query_collection(\n        self,\n        queries: list[str],\n        n_results: int = 3,\n    ) -> chroma_types.QueryResult | None:\n        \"\"\"\n        Queries the Chroma collection manager with a list of queries.\n\n        Args:\n            - queries (list[str]): List of queries to use in the Chroma collection manager.\n            - n_results (int, optional): Number of results to return. Defaults to 3.\n\n        Returns:\n            - chroma_types.QueryResult | None: The result of the Chroma query, or None if unsuccessful.\n        \"\"\"\n\n        return self.collection_manager.query_collection(\n            queries,\n            n_results=n_results,\n            include_in_result=[\"metadatas\", \"documents\"],\n        )\n\n    def _get_chroma_queries(\n        self, user_question: str, queries_count: int = 3, retries: int = 3\n    ) -> list[str] | None:\n        \"\"\"\n        Generates Chroma queries based on the user question.\n\n        Args:\n            - user_question (str): The user's question.\n            - queries_count (int, optional): Number of queries to generate. Defaults to 3.\n            - retries (int, optional): Number of retries in case of failure. Defaults to 3.\n\n        Returns:\n            - list[str] | None: The generated list of Chroma queries, or None if unsuccessful.\n        \"\"\"\n\n        while retries > 0:\n            retries -= 1\n\n            prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n                user_question,\n                prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n                queries_count=queries_count,\n            )\n\n            try:\n                completion: openai_types.ChatCompletion = (\n                    self.client.chat.completions.create(\n                        model=self.model,\n                        response_format={\"type\": \"json_object\"},\n                        messages=[\n                            {\n                                \"role\": \"system\",\n                                \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                            },\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                    )\n                )\n                content: str | None = completion.choices[0].message.content\n                if not content:\n                    continue\n\n                content_json = json.loads(content)\n                content_model = OpenAIResponseContent(\n                    query_list=content_json[\"query_list\"]\n                )\n                content_model.query_list.append(user_question)\n                queries_count += 1\n\n                if content:\n                    queries: list[str] = content_model.query_list\n                    if queries and len(queries) == queries_count:\n                        return queries\n\n            except Exception as e:\n                logging.error(f\"An error occurred: {e}\")\n\n        return None\n",
    "important_comments": null,
    "dependencies": null,
    "summary": "This code is designed to enhance the interaction between natural language inputs and a Chroma vector database by leveraging OpenAI's language models to generate and execute queries. Its primary purpose is to facilitate the retrieval of relevant data from the Chroma database based on user input, thereby improving the search and retrieval process within systems that require efficient data handling and natural language processing capabilities. The key components include the `OpenAIResponseContent` class, a Pydantic model that ensures the structured parsing of JSON responses from OpenAI's API, focusing on maintaining data integrity by validating the list of queries returned. The `ChromaLibrarian` class orchestrates the interaction with both the Chroma database and OpenAI, providing methods such as `query_chroma` for processing user questions, `_query_collection` for executing queries against the Chroma collection manager, and `_get_chroma_queries` for generating queries using OpenAI's language models.\n\nThe implementation involves a robust mechanism for generating and executing queries. The `ChromaLibrarian` class initializes with a `ChromaCollectionManager` and an OpenAI model, defaulting to \"gpt-3.5-turbo-1106\". The `query_chroma` method initiates the process by taking a user question, generating potential queries through `_get_chroma_queries`, and retrieving results via `_query_collection`. The `_get_chroma_queries` method employs a retry mechanism to ensure reliable query generation, using OpenAI's API to create chat completions and parsing the JSON response to extract a list of queries. This method dynamically adjusts the number of queries based on user input and retry attempts, ensuring robustness. The `_query_collection` method interacts with the `ChromaCollectionManager` to fetch results, specifying the inclusion of metadata and documents to enrich the returned data.\n\nThe technical stack includes OpenAI's API for generating language model completions, which provides the natural language processing capabilities necessary for query generation. Pydantic is used for data validation and parsing, ensuring that the JSON responses are correctly structured. The Fenec library is utilized for managing Chroma database collections and prompt creation, with components like `ChromaCollectionManager` and `ChromaLibrarianPromptCreator` playing crucial roles in database interaction and prompt generation, respectively.\n\nIn the context of a larger system, this code acts as a critical bridge between natural language input and structured database queries, integrating seamlessly with other components such as the Chroma database and OpenAI's services. It fits into a broader architecture where it likely interfaces with user-facing applications or services that require efficient data retrieval from vector databases, enhancing the overall functionality of systems that rely on natural language processing and vector search capabilities. This code is essential for applications like recommendation systems, search engines, or AI-driven analytics platforms, where accurate and efficient data processing is paramount. It interacts with other modules responsible for managing database connections, executing queries, and interfacing with external APIs, forming a vital part of a data processing pipeline that supports advanced AI functionalities.",
    "children_ids": [
        "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-OpenAIResponseContent",
        "fenec:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian"
    ]
}