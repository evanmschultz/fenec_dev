{
    "docstring": null,
    "header": [],
    "footer": [],
    "imports": [
        {
            "import_names": [
                {
                    "name": "logging",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "json",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAI",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "openai",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "BaseModel",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "pydantic",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "postcode.types.openai",
                    "as_name": "openai_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:types:openai.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaCollectionManager",
                    "as_name": null,
                    "local_block_id": "postcode:databases:chroma:chromadb_collection_manager.py__*__MODULE__*__CLASS-ChromaCollectionManager"
                }
            ],
            "imported_from": "postcode.databases.chroma.chromadb_collection_manager",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:databases:chroma:chromadb_collection_manager.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarianPromptCreator",
                    "as_name": null,
                    "local_block_id": "postcode:ai_services:librarians:prompts:prompt_creator.py__*__MODULE__*__CLASS-ChromaLibrarianPromptCreator"
                }
            ],
            "imported_from": "postcode.ai_services.librarians.prompts.prompt_creator",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:librarians:prompts:prompt_creator.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "postcode.ai_services.librarians.prompts.chroma_librarian_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:librarians:prompts:chroma_librarian_prompts.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "postcode.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:types:chroma.py__*__MODULE"
        }
    ],
    "id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE",
    "file_path": "postcode/ai_services/librarians/chroma_librarians.py",
    "parent_id": "postcode:ai_services:librarians__*__DIRECTORY",
    "block_type": "MODULE",
    "start_line_num": 1,
    "end_line_num": 152,
    "code_content": "import logging\nimport json\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport postcode.types.openai as openai_types\nfrom postcode.databases.chroma.chromadb_collection_manager import (\n    ChromaCollectionManager,\n)\nfrom postcode.ai_services.librarians.prompts.prompt_creator import (\n    ChromaLibrarianPromptCreator,\n)\nfrom postcode.ai_services.librarians.prompts.chroma_librarian_prompts import (\n    DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n    DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n)\nimport postcode.types.chroma as chroma_types\n\n# TOOLS: list[dict[str, Any]] = [\n#     {\n#         \"type\": \"function\",\n#         \"function\": {\n#             \"name\": \"query_chroma\",\n#             \"description\": \"Get the results from the chromadb vector database using a list of queries.\",\n#             \"parameters\": {\n#                 \"type\": \"object\",\n#                 \"properties\": {\n#                     \"queries\": {\n#                         \"type\": \"list[str]\",\n#                         \"description\": \"List of queries to use to get the results from the chromadb vector database.\",\n#                     },\n#                     \"n_results\": {\n#                         \"type\": \"int\",\n#                         \"description\": \"Number of results to return, default is 10.\",\n#                     },\n#                 },\n#                 \"required\": [\"queries\"],\n#             },\n#         },\n#     }\n# ]\n\n\nclass OpenAIResponseContent(BaseModel):\n    query_list: list[str]\n\n\nclass ChromaLibrarian:\n    def __init__(\n        self,\n        collection_manager: ChromaCollectionManager,\n        model: str = \"gpt-3.5-turbo-1106\",\n    ) -> None:\n        self.collection_manager: ChromaCollectionManager = collection_manager\n        self.model: str = model\n        self.client = OpenAI()\n\n    def query_chroma(self, user_question: str) -> chroma_types.QueryResult | None:\n        queries: list[str] | None = self._get_chroma_queries(user_question)\n        if not queries:\n            return None\n\n        print(queries)\n\n        return self._query_collection(queries)\n\n    def _query_collection(\n        self,\n        queries: list[str],\n        n_results: int = 3,\n    ) -> chroma_types.QueryResult | None:\n        return self.collection_manager.query_collection(\n            queries,\n            n_results=n_results,\n            include_in_result=[\"metadatas\", \"documents\"],\n        )\n\n    def _get_chroma_queries(\n        self, user_question: str, queries_count: int = 3, retries: int = 3\n    ) -> list[str] | None:\n        while retries > 0:\n            retries -= 1\n\n            prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n                user_question,\n                prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n                queries_count=queries_count,\n            )\n\n            try:\n                completion: openai_types.ChatCompletion = (\n                    self.client.chat.completions.create(\n                        model=self.model,\n                        response_format={\"type\": \"json_object\"},\n                        messages=[\n                            {\n                                \"role\": \"system\",\n                                \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                            },\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                    )\n                )\n                content: str | None = completion.choices[0].message.content\n                if not content:\n                    continue\n\n                content_json = json.loads(content)\n                content_model = OpenAIResponseContent(\n                    query_list=content_json[\"query_list\"]\n                )\n                print(content_model)\n\n                if content:\n                    queries: list[str] = content_model.query_list\n                    if queries and len(queries) == queries_count:\n                        return queries\n\n            except Exception as e:\n                logging.error(f\"An error occurred: {e}\")\n\n        return None\n\n    # def get_chroma_queries(\n    #     self, user_question: str, queries_count: int = 3, retries: int = 3\n    # ) -> list[str] | None:\n    #     if retries < 1:\n    #         return None\n\n    #     prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n    #         user_question,\n    #         prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n    #         queries_count=queries_count,\n    #     )\n\n    #     completion: openai_types.ChatCompletion = self.client.chat.completions.create(\n    #         model=self.model,\n    #         messages=[\n    #             {\"role\": \"system\", \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT},\n    #             {\"role\": \"user\", \"content\": prompt},\n    #         ],\n    #     )\n    #     content: str | None = completion.choices[0].message.content\n    #     if not content:\n    #         return self.get_chroma_queries(user_question, queries_count, retries - 1)\n\n    #     queries: str = content.split(\"QUERIES_LIST:\")[-1]\n    #     if len(queries) < 1:\n    #         return self.get_chroma_queries(user_question, queries_count, retries - 1)\n\n    #     return queries.strip(\"[]\").split(\", \")\n",
    "important_comments": null,
    "dependencies": null,
    "summary": "\nSummary:\n\n        postcode:ai_services:librarians:chroma_librarians.py__*__MODULE\n\n        \n\nSummary:\n\n        postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-OpenAIResponseContent\n\n        None, None, \nfrom openai import OpenAI\nfrom pydantic import BaseModel\n        \n\nSummary:\n\n        postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian\n\n        \nChild (postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-__init__) code content:\ndef __init__(\n    self,\n    collection_manager: ChromaCollectionManager,\n    model: str = \"gpt-3.5-turbo-1106\",\n) -> None:\n    self.collection_manager: ChromaCollectionManager = collection_manager\n    self.model: str = model\n    self.client = OpenAI()\n\n\nChild (postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-query_chroma) code content:\n\ndef query_chroma(self, user_question: str) -> chroma_types.QueryResult | None:\n    queries: list[str] | None = self._get_chroma_queries(user_question)\n    if not queries:\n        return None\n\n    print(queries)\n\n    return self._query_collection(queries)\n\n\nChild (postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-_query_collection) code content:\n\ndef _query_collection(\n    self,\n    queries: list[str],\n    n_results: int = 3,\n) -> chroma_types.QueryResult | None:\n    return self.collection_manager.query_collection(\n        queries,\n        n_results=n_results,\n        include_in_result=[\"metadatas\", \"documents\"],\n    )\n\n\nChild (postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-_get_chroma_queries) code content:\n\ndef _get_chroma_queries(\n    self, user_question: str, queries_count: int = 3, retries: int = 3\n) -> list[str] | None:\n    while retries > 0:\n        retries -= 1\n\n        prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n            user_question,\n            prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n            queries_count=queries_count,\n        )\n\n        try:\n            completion: openai_types.ChatCompletion = (\n                self.client.chat.completions.create(\n                    model=self.model,\n                    response_format={\"type\": \"json_object\"},\n                    messages=[\n                        {\n                            \"role\": \"system\",\n                            \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                        },\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                )\n            )\n            content: str | None = completion.choices[0].message.content\n            if not content:\n                continue\n\n            content_json = json.loads(content)\n            content_model = OpenAIResponseContent(\n                query_list=content_json[\"query_list\"]\n            )\n            print(content_model)\n\n            if content:\n                queries: list[str] = content_model.query_list\n                if queries and len(queries) == queries_count:\n                    return queries\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n    return None\n\n, None, None\n        , None, None\n        ",
    "children_ids": [
        "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-OpenAIResponseContent",
        "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian"
    ]
}