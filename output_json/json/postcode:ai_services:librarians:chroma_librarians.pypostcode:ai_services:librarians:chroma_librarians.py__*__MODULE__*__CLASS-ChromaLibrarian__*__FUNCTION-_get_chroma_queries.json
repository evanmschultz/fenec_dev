{
    "function_name": "_get_chroma_queries",
    "docstring": "Generates Chroma queries based on the user question.\n\nArgs:\n    - user_question (str): The user's question.\n    - queries_count (int, optional): Number of queries to generate. Defaults to 3.\n    - retries (int, optional): Number of retries in case of failure. Defaults to 3.\n\nReturns:\n    - list[str] | None: The generated list of Chroma queries, or None if unsuccessful.",
    "decorators": null,
    "parameters": null,
    "returns": "list[str] | None",
    "is_method": true,
    "is_async": false,
    "id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-_get_chroma_queries",
    "file_path": "postcode/ai_services/librarians/chroma_librarians.py",
    "parent_id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian",
    "block_type": "FUNCTION",
    "start_line_num": 136,
    "end_line_num": 193,
    "code_content": "\ndef _get_chroma_queries(\n    self, user_question: str, queries_count: int = 3, retries: int = 3\n) -> list[str] | None:\n    \"\"\"\n        Generates Chroma queries based on the user question.\n\n        Args:\n            - user_question (str): The user's question.\n            - queries_count (int, optional): Number of queries to generate. Defaults to 3.\n            - retries (int, optional): Number of retries in case of failure. Defaults to 3.\n\n        Returns:\n            - list[str] | None: The generated list of Chroma queries, or None if unsuccessful.\n        \"\"\"\n\n    while retries > 0:\n        retries -= 1\n\n        prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n            user_question,\n            prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n            queries_count=queries_count,\n        )\n\n        try:\n            completion: openai_types.ChatCompletion = (\n                self.client.chat.completions.create(\n                    model=self.model,\n                    response_format={\"type\": \"json_object\"},\n                    messages=[\n                        {\n                            \"role\": \"system\",\n                            \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                        },\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                )\n            )\n            content: str | None = completion.choices[0].message.content\n            if not content:\n                continue\n\n            content_json = json.loads(content)\n            content_model = OpenAIResponseContent(\n                query_list=content_json[\"query_list\"]\n            )\n\n            if content:\n                queries: list[str] = content_model.query_list\n                if queries and len(queries) == queries_count:\n                    return queries\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n    return None\n",
    "important_comments": null,
    "dependencies": null,
    "summary": "\nThe `_get_chroma_queries` method generates a list of Chroma queries by constructing messages for the OpenAI API, using a prompt creator with predefined templates. It sends these messages, parses the JSON response to extract queries, and ensures the correct number of queries is returned. The method retries upon failure, logs exceptions, and returns either the queries as a list of strings or `None` if it fails after all retries.",
    "children_ids": []
}