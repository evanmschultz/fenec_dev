{
    "class_name": "OpenAIChatAgent",
    "decorators": null,
    "bases": null,
    "docstring": "Represents an agent that interacts with the OpenAI API for generating responses to user questions.\n\nArgs:\n    - chroma_librarian (ChromaLibrarian): The librarian handling Chroma queries.\n    - model (str, optional): The OpenAI model to use. Defaults to \"gpt-4-1106-preview\".\n\nMethods:\n    - get_response(user_question, prompt_template=DEFAULT_PROMPT_TEMPLATE):\n        Generates a response to the user's question using the specified prompt template.\n\n    - _format_prompt(context, user_question, prompt_template):\n        Formats the prompt for the OpenAI API based on the context and user's question.\n\nAttributes:\n    - chroma_librarian (ChromaLibrarian): The Chroma librarian instance.\n    - model (str): The OpenAI model being used.\n    - client: The OpenAI API client.",
    "keywords": null,
    "id": "postcode:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent",
    "file_path": "postcode/ai_services/chat/openai_agents.py",
    "parent_id": "postcode:ai_services:chat:openai_agents.py__*__MODULE",
    "block_type": "CLASS",
    "start_line_num": 13,
    "end_line_num": 148,
    "code_content": "\n\nclass OpenAIChatAgent:\n    \"\"\"\n    Represents an agent that interacts with the OpenAI API for generating responses to user questions.\n\n    Args:\n        - chroma_librarian (ChromaLibrarian): The librarian handling Chroma queries.\n        - model (str, optional): The OpenAI model to use. Defaults to \"gpt-4-1106-preview\".\n\n    Methods:\n        - get_response(user_question, prompt_template=DEFAULT_PROMPT_TEMPLATE):\n            Generates a response to the user's question using the specified prompt template.\n\n        - _format_prompt(context, user_question, prompt_template):\n            Formats the prompt for the OpenAI API based on the context and user's question.\n\n    Attributes:\n        - chroma_librarian (ChromaLibrarian): The Chroma librarian instance.\n        - model (str): The OpenAI model being used.\n        - client: The OpenAI API client.\n    \"\"\"\n\n    def __init__(\n        self,\n        chroma_librarian: ChromaLibrarian,\n        model: str = \"gpt-4-1106-preview\",\n    ) -> None:\n        self.chroma_librarian: ChromaLibrarian = chroma_librarian\n        self.model: str = model\n        self.client = OpenAI()\n\n    def get_response(\n        self, user_question: str, prompt_template: str = DEFAULT_PROMPT_TEMPLATE\n    ) -> str | None:\n        \"\"\"\n        Generates a response to the user's question using the OpenAI API.\n\n        Args:\n            - user_question (str): The user's question.\n            - prompt_template (str, optional): The template for formatting the prompt.\n                Defaults to DEFAULT_PROMPT_TEMPLATE.\n\n        Returns:\n            - str | None: The generated response or None if the response could not be generated.\n\n        Raises:\n            - ValueError: If user_question is empty.\n            - RuntimeError: If there is an issue with the OpenAI API request.\n            - KeyError: If the prompt template is missing required keys.\n\n        Example:\n            ```python\n            agent = OpenAIChatAgent(chroma_librarian, model=\"gpt-4-1106-preview\")\n            try:\n                response = agent.get_response(\"What is the capital of France?\")\n                print(response)\n            except ValueError as ve:\n                print(f\"ValueError: {ve}\")\n            except RuntimeError as re:\n                print(f\"RuntimeError: {re}\")\n            except KeyError as ke:\n                print(f\"KeyError: {ke}\")\n            ```\n        \"\"\"\n        if not user_question:\n            raise ValueError(\"User question cannot be empty.\")\n\n        try:\n            chroma_results: chroma_types.QueryResult | None = (\n                self.chroma_librarian.query_chroma(user_question)\n            )\n\n            if not chroma_results:\n                return \"I don't know how to answer that question.\"\n\n            documents: list[list[str]] | None = chroma_results[\"documents\"]\n\n            if not documents:\n                return \"I don't know how to answer that question.\"\n\n            context: str = \"\"\n            for document in documents:\n                context += \"\\n\".join(document) + \"\\n\"\n\n            prompt: str = self._format_prompt(context, user_question, prompt_template)\n\n            messages: Sequence[dict[str, str]] = [\n                {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n            response: openai_types.ChatCompletion = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,  # type: ignore # FIXME: fix type hinting error\n                temperature=0.1,\n                # response_format={\"type\": \"json_object\"},\n            )\n            return response.choices[0].message.content\n\n        except Exception as e:\n            raise RuntimeError(f\"Error interacting with OpenAI API: {e}\") from e\n\n    def _format_prompt(\n        self,\n        context: str,\n        user_question: str,\n        prompt_template: str,\n    ) -> str:\n        \"\"\"\n        Formats the prompt for the OpenAI API based on the provided context, user's question, and template.\n\n        Args:\n            - context (str): The context derived from Chroma query results.\n            - user_question (str): The user's question.\n            - prompt_template (str): The template for formatting the prompt.\n\n        Returns:\n            - str: The formatted prompt.\n\n        Raises:\n            - KeyError: If the prompt template is missing required keys.\n\n        Example:\n            ```python\n            prompt = agent._format_prompt(\"Context here\", \"What is the meaning of life?\", \"Template {context} {user_question}\")\n            print(prompt)\n            ```\n        \"\"\"\n\n        try:\n            return prompt_template.format(context=context, user_question=user_question)\n\n        except KeyError as e:\n            raise KeyError(f\"Prompt template is missing the following key: {e}\") from e\n",
    "important_comments": null,
    "dependencies": [
        {
            "import_names": [
                {
                    "name": "Sequence",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "typing",
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAI",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "openai",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "postcode.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:types:chroma.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "postcode.types.openai",
                    "as_name": "openai_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:types:openai.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarian",
                    "as_name": null,
                    "local_block_id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian"
                }
            ],
            "imported_from": "postcode.ai_services.librarians.chroma_librarians",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_PROMPT_TEMPLATE",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "postcode.ai_services.chat.prompts.prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:chat:prompts:prompts.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_PROMPT_TEMPLATE",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "postcode.ai_services.chat.prompts.prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:chat:prompts:prompts.py__*__MODULE"
        }
    ],
    "summary": "\n\nThe `OpenAIChatAgent` class is designed to generate responses to user questions by interfacing with the OpenAI API, utilizing a `ChromaLibrarian` instance for context retrieval and a specified OpenAI model. The class is initialized with a `chroma_librarian` for Chroma queries, a model string defaulting to \"gpt-4-1106-preview\", and an OpenAI client. The `get_response` method takes a user question and an optional prompt template to query the ChromaLibrarian for relevant documents, construct a context, and format a prompt for the OpenAI API. It handles various exceptions, including empty questions, API errors, and missing template keys, and notes a type hinting error to be addressed. The `_format_prompt` method assists in creating the prompt by inserting the context and user question into the provided template, raising a `KeyError` with a detailed message if required keys are missing. The class encapsulates the logic for interacting with the OpenAI API, managing the flow from question to context gathering to response generation, while providing robust error handling and informative exceptions.",
    "children_ids": [
        "postcode:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent__*__FUNCTION-__init__",
        "postcode:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent__*__FUNCTION-get_response",
        "postcode:ai_services:chat:openai_agents.py__*__MODULE__*__CLASS-OpenAIChatAgent__*__FUNCTION-_format_prompt"
    ]
}