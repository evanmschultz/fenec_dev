{
    "function_name": "test_query_chroma_returns_none_when_no_queries",
    "docstring": null,
    "decorators": null,
    "parameters": null,
    "returns": "None",
    "is_method": false,
    "is_async": false,
    "id": "postcode:tests:ai_services:librarians:test_chroma_librarians.py__*__MODULE__*__FUNCTION-test_query_chroma_returns_none_when_no_queries",
    "file_path": "postcode/tests/ai_services/librarians/test_chroma_librarians.py",
    "parent_id": "postcode:tests:ai_services:librarians:test_chroma_librarians.py__*__MODULE",
    "block_type": "FUNCTION",
    "start_line_num": 46,
    "end_line_num": 70,
    "code_content": "\n\ndef test_query_chroma_returns_none_when_no_queries(\n    mock_collection_manager: Mock, mock_openai_client: Mock\n) -> None:\n    librarian = ChromaLibrarian(mock_collection_manager)\n    librarian.client = mock_openai_client\n\n    mock_openai_client.chat.completions.create.return_value = {\n        \"choices\": [{\"message\": {\"content\": None}}]\n    }\n\n    result: chroma_types.QueryResult | None = librarian.query_chroma(\"question\")\n\n    assert result is None\n    mock_openai_client.chat.completions.create.assert_called_once_with(\n        model=\"gpt-3.5-turbo-1106\",\n        response_format={\"type\": \"json_object\"},\n        messages=[\n            {\"role\": \"system\", \"content\": \"default_system_prompt\"},\n            {\"role\": \"user\", \"content\": \"prompt\"},\n        ],\n    )\n    mock_collection_manager.query_collection.assert_not_called()\n",
    "important_comments": null,
    "dependencies": [
        {
            "code_block_id": "postcode:tests:ai_services:librarians:test_chroma_librarians.py__*__MODULE__*__FUNCTION-mock_collection_manager"
        },
        {
            "code_block_id": "postcode:tests:ai_services:librarians:test_chroma_librarians.py__*__MODULE__*__FUNCTION-mock_openai_client"
        },
        {
            "import_names": [
                {
                    "name": "Mock",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "unittest.mock",
            "import_module_type": "LOCAL",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarian",
                    "as_name": null,
                    "local_block_id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian"
                }
            ],
            "imported_from": "postcode.ai_services.librarians.chroma_librarians",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "postcode.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:types:chroma.py__*__MODULE"
        }
    ],
    "summary": "\nSummary:\n\n        postcode:tests:ai_services:librarians:test_chroma_librarians.py__*__MODULE__*__FUNCTION-test_query_chroma_returns_none_when_no_queries\n\n        None, \nImported code block (postcode:ai_services:librarians:chroma_librarians.py__*__MODULE) code content:\n\n\nclass ChromaLibrarian:\n    def __init__(\n        self,\n        collection_manager: ChromaCollectionManager,\n        model: str = \"gpt-3.5-turbo-1106\",\n    ) -> None:\n        self.collection_manager: ChromaCollectionManager = collection_manager\n        self.model: str = model\n        self.client = OpenAI()\n\n    def query_chroma(self, user_question: str) -> chroma_types.QueryResult | None:\n        queries: list[str] | None = self._get_chroma_queries(user_question)\n        if not queries:\n            return None\n\n        print(queries)\n\n        return self._query_collection(queries)\n\n    def _query_collection(\n        self,\n        queries: list[str],\n        n_results: int = 3,\n    ) -> chroma_types.QueryResult | None:\n        return self.collection_manager.query_collection(\n            queries,\n            n_results=n_results,\n            include_in_result=[\"metadatas\", \"documents\"],\n        )\n\n    def _get_chroma_queries(\n        self, user_question: str, queries_count: int = 3, retries: int = 3\n    ) -> list[str] | None:\n        while retries > 0:\n            retries -= 1\n\n            prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n                user_question,\n                prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n                queries_count=queries_count,\n            )\n\n            try:\n                completion: openai_types.ChatCompletion = (\n                    self.client.chat.completions.create(\n                        model=self.model,\n                        response_format={\"type\": \"json_object\"},\n                        messages=[\n                            {\n                                \"role\": \"system\",\n                                \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                            },\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                    )\n                )\n                content: str | None = completion.choices[0].message.content\n                if not content:\n                    continue\n\n                content_json = json.loads(content)\n                content_model = OpenAIResponseContent(\n                    query_list=content_json[\"query_list\"]\n                )\n                print(content_model)\n\n                if content:\n                    queries: list[str] = content_model.query_list\n                    if queries and len(queries) == queries_count:\n                        return queries\n\n            except Exception as e:\n                logging.error(f\"An error occurred: {e}\")\n\n        return None\n\n    # def get_chroma_queries(\n    #     self, user_question: str, queries_count: int = 3, retries: int = 3\n    # ) -> list[str] | None:\n    #     if retries < 1:\n    #         return None\n\n    #     prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n    #         user_question,\n    #         prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n    #         queries_count=queries_count,\n    #     )\n\n    #     completion: openai_types.ChatCompletion = self.client.chat.completions.create(\n    #         model=self.model,\n    #         messages=[\n    #             {\"role\": \"system\", \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT},\n    #             {\"role\": \"user\", \"content\": prompt},\n    #         ],\n    #     )\n    #     content: str | None = completion.choices[0].message.content\n    #     if not content:\n    #         return self.get_chroma_queries(user_question, queries_count, retries - 1)\n\n    #     queries: str = content.split(\"QUERIES_LIST:\")[-1]\n    #     if len(queries) < 1:\n    #         return self.get_chroma_queries(user_question, queries_count, retries - 1)\n\n    #     return queries.strip(\"[]\").split(\", \")\n\n, \n        ",
    "children_ids": []
}