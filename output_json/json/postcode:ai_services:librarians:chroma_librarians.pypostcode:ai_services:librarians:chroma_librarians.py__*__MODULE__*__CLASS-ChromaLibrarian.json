{
    "class_name": "ChromaLibrarian",
    "decorators": null,
    "bases": null,
    "docstring": null,
    "keywords": null,
    "id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian",
    "file_path": "postcode/ai_services/librarians/chroma_librarians.py",
    "parent_id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE",
    "block_type": "CLASS",
    "start_line_num": 55,
    "end_line_num": 193,
    "code_content": "\n\nclass ChromaLibrarian:\n    def __init__(\n        self,\n        collection_manager: ChromaCollectionManager,\n        model: str = \"gpt-3.5-turbo-1106\",\n    ) -> None:\n        \"\"\"\n        Represents a librarian for interacting with the Chroma database using OpenAI.\n\n        Args:\n            - collection_manager (ChromaCollectionManager): The manager for Chroma collections.\n            - model (str, optional): The OpenAI model to use. Defaults to \"gpt-3.5-turbo-1106\".\n\n        Methods:\n            - query_chroma(user_question):\n                Queries the Chroma database using the provided user question.\n\n            - _query_collection(queries, n_results=3):\n                Queries the Chroma collection manager with a list of queries.\n\n            - _get_chroma_queries(user_question, queries_count=3, retries=3):\n                Generates Chroma queries based on the user question.\n\n        Attributes:\n            - collection_manager (ChromaCollectionManager): The Chroma collection manager.\n            - model (str): The OpenAI model being used.\n            - client: The OpenAI API client.\n\n        Examples:\n            ```python\n            chroma_librarian = ChromaLibrarian(chroma_collection_manager)\n            chroma_librarian.query_chroma(\"Which models are inherited by others?\")\n            ```\n        \"\"\"\n\n        self.collection_manager: ChromaCollectionManager = collection_manager\n        self.model: str = model\n        self.client = OpenAI()\n\n    def query_chroma(self, user_question: str) -> chroma_types.QueryResult | None:\n        \"\"\"\n        Queries the Chroma database using the provided user question.\n\n        Args:\n            - user_question (str): The user's question.\n\n        Returns:\n            - chroma_types.QueryResult | None: The result of the Chroma query, or None if unsuccessful.\n        \"\"\"\n\n        queries: list[str] | None = self._get_chroma_queries(user_question)\n        if not queries:\n            return None\n\n        print(queries)\n\n        return self._query_collection(queries)\n\n    def _query_collection(\n        self,\n        queries: list[str],\n        n_results: int = 3,\n    ) -> chroma_types.QueryResult | None:\n        \"\"\"\n        Queries the Chroma collection manager with a list of queries.\n\n        Args:\n            - queries (list[str]): List of queries to use in the Chroma collection manager.\n            - n_results (int, optional): Number of results to return. Defaults to 3.\n\n        Returns:\n            - chroma_types.QueryResult | None: The result of the Chroma query, or None if unsuccessful.\n        \"\"\"\n\n        return self.collection_manager.query_collection(\n            queries,\n            n_results=n_results,\n            include_in_result=[\"metadatas\", \"documents\"],\n        )\n\n    def _get_chroma_queries(\n        self, user_question: str, queries_count: int = 3, retries: int = 3\n    ) -> list[str] | None:\n        \"\"\"\n        Generates Chroma queries based on the user question.\n\n        Args:\n            - user_question (str): The user's question.\n            - queries_count (int, optional): Number of queries to generate. Defaults to 3.\n            - retries (int, optional): Number of retries in case of failure. Defaults to 3.\n\n        Returns:\n            - list[str] | None: The generated list of Chroma queries, or None if unsuccessful.\n        \"\"\"\n\n        while retries > 0:\n            retries -= 1\n\n            prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n                user_question,\n                prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n                queries_count=queries_count,\n            )\n\n            try:\n                completion: openai_types.ChatCompletion = (\n                    self.client.chat.completions.create(\n                        model=self.model,\n                        response_format={\"type\": \"json_object\"},\n                        messages=[\n                            {\n                                \"role\": \"system\",\n                                \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                            },\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                    )\n                )\n                content: str | None = completion.choices[0].message.content\n                if not content:\n                    continue\n\n                content_json = json.loads(content)\n                content_model = OpenAIResponseContent(\n                    query_list=content_json[\"query_list\"]\n                )\n\n                if content:\n                    queries: list[str] = content_model.query_list\n                    if queries and len(queries) == queries_count:\n                        return queries\n\n            except Exception as e:\n                logging.error(f\"An error occurred: {e}\")\n\n        return None\n",
    "important_comments": null,
    "dependencies": [
        {
            "code_block_id": "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-OpenAIResponseContent"
        },
        {
            "import_names": [
                {
                    "name": "logging",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "json",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "OpenAI",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "openai",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": null
        },
        {
            "import_names": [
                {
                    "name": "postcode.types.openai",
                    "as_name": "openai_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:types:openai.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaCollectionManager",
                    "as_name": null,
                    "local_block_id": "postcode:databases:chroma:chromadb_collection_manager.py__*__MODULE__*__CLASS-ChromaCollectionManager"
                }
            ],
            "imported_from": "postcode.databases.chroma.chromadb_collection_manager",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:databases:chroma:chromadb_collection_manager.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarianPromptCreator",
                    "as_name": null,
                    "local_block_id": "postcode:ai_services:librarians:prompts:prompt_creator.py__*__MODULE__*__CLASS-ChromaLibrarianPromptCreator"
                }
            ],
            "imported_from": "postcode.ai_services.librarians.prompts.prompt_creator",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:librarians:prompts:prompt_creator.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "postcode.ai_services.librarians.prompts.chroma_librarian_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:librarians:prompts:chroma_librarian_prompts.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                },
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT",
                    "as_name": null,
                    "local_block_id": null
                }
            ],
            "imported_from": "postcode.ai_services.librarians.prompts.chroma_librarian_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:ai_services:librarians:prompts:chroma_librarian_prompts.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "postcode.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": null
                }
            ],
            "imported_from": null,
            "import_module_type": "LOCAL",
            "local_module_id": "postcode:types:chroma.py__*__MODULE"
        }
    ],
    "summary": "\n\nThe `ChromaLibrarian` class in Python is designed to interact with the Chroma database using OpenAI's GPT-3.5 Turbo model by default. It is initialized with a `ChromaCollectionManager` and an optional model specification. The class provides three main functionalities: `query_chroma`, `_query_collection`, and `_get_chroma_queries`. The `query_chroma` method takes a user's question and attempts to generate relevant queries using `_get_chroma_queries`, which creates prompts for the OpenAI model and processes the responses to form a list of queries. If successful, these queries are passed to `_query_collection`, which interacts with the `ChromaCollectionManager` to fetch and return the query results, including metadata and documents. The class handles exceptions and retries query generation, ensuring robustness in case of failures.",
    "children_ids": [
        "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-__init__",
        "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-query_chroma",
        "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-_query_collection",
        "postcode:ai_services:librarians:chroma_librarians.py__*__MODULE__*__CLASS-ChromaLibrarian__*__FUNCTION-_get_chroma_queries"
    ]
}